import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import os
import math
import time
import numpy as np
from tqdm import tqdm
import torch.nn.functional as F
from collections import Counter
import json
import gc

# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    system = "my"
    
    if system == "my":
        dir = "/media/alex/Programs/NEURAL_NETWORKS/"
    else:
        dir = "/content/"
    
    # –ü—É—Ç–∏
    data_path = dir + "NeuralNetwork-GPT/DataSet.txt"    # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å —Ñ—Ä–∞–∑–∞–º–∏
    model_path = dir + "NeuralNetwork-GPT/Model/text_model.pth"  # –ü—É—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏
    vocab_path = dir + "NeuralNetwork-GPT/Model/vocab.json"      # –°–ª–æ–≤–∞—Ä—å
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    d_model = 512              # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    nhead = 8                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
    num_layers = 6             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
    dim_feedforward = 2048     # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è FFN
    max_seq_len = 256          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    dropout = 0.5              # Dropout
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    batch_size = 32            # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    lr = 0.0001                # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    epochs = 20                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    accumulation_steps = 4     # –®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    early_stop_patience = 3    # –¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    min_loss_delta = 0.001     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ loss
    mixed_precision = True     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision

config = Config()

# ====================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ======================
class TextDataset(Dataset):
    def __init__(self, texts, vocab, seq_len):
        self.texts = texts
        self.vocab = vocab
        self.seq_len = seq_len
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        tokens = text.split()[:self.seq_len]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∏–Ω–¥–µ–∫—Å—ã
        input_ids = [self.vocab.get(token, self.vocab["<unk>"]) for token in tokens]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞
        if len(input_ids) < self.seq_len:
            input_ids += [self.vocab["<pad>"]] * (self.seq_len - len(input_ids))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è
        attention_mask = [1] * len(tokens) + [0] * (self.seq_len - len(tokens))
        
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.bool)
        }

def build_vocab(texts, min_freq=3):
    counter = Counter()
    for text in texts:
        counter.update(text.split())
    
    vocab = {
        "<pad>": 0,
        "<unk>": 1,
        "<sos>": 2,
        "<eos>": 3
    }
    
    idx = 4
    for word, count in counter.items():
        if count >= min_freq:
            vocab[word] = idx
            idx += 1
    
    return vocab

def load_data(val_split=0.1):
    if not os.path.exists(config.data_path):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config.data_path}")
    
    with open(config.data_path, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]
    
    if len(texts) == 0:
        raise ValueError(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—É—Å—Ç: {config.data_path}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    val_size = int(len(texts) * val_split)
    train_size = len(texts) - val_size
    train_texts, val_texts = random_split(texts, [train_size, val_size])
    
    return list(train_texts), list(val_texts)

# ====================== –ú–û–î–ï–õ–¨ ======================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x.transpose(0, 1)
        x = x + self.pe[:x.size(0)]
        return self.dropout(x).transpose(0, 1)

class TextTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model, nhead, dim_feedforward, dropout, 
            batch_first=True,
            device=torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        self.fc_out = nn.Linear(d_model, vocab_size)
    
    def forward(self, src, src_mask=None, src_key_padding_mask=None):
        src = self.embedding(src)
        src = self.pos_encoder(src)
        
        output = self.transformer(
            src, 
            mask=src_mask,
            src_key_padding_mask=src_key_padding_mask
        )
        
        return self.fc_out(output)

# ====================== –û–ë–£–ß–ï–ù–ò–ï ======================
def train_epoch(model, loader, optimizer, device, scheduler=None, scaler=None):
    model.train()
    total_loss = 0
    optimizer.zero_grad()
    
    for i, batch in enumerate(tqdm(loader)):
        inputs = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã (—Å–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω)
        targets = inputs[:, 1:].contiguous()
        inputs = inputs[:, :-1]
        mask = mask[:, :-1]
        
        autocast_context = torch.amp.autocast(
            device_type='cuda', 
            dtype=torch.float16, 
            enabled=scaler is not None and config.mixed_precision
        )
        
        with autocast_context:
            outputs = model(inputs, src_key_padding_mask=~mask)
            
            # –ü–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å
            outputs = outputs.view(-1, outputs.size(-1))
            targets = targets.view(-1)
            
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥ –≤ —Ç–∞—Ä–≥–µ—Ç–∞—Ö
            loss = F.cross_entropy(
                outputs,
                targets,
                ignore_index=0  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥
            ) / config.accumulation_steps
        
        if scaler:
            scaler.scale(loss).backward()
        else:
            loss.backward()
        
        if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(loader):
            # Gradient clipping
            if scaler:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
            optimizer.zero_grad(set_to_none=True)
            if scheduler:
                scheduler.step(total_loss)
        
        total_loss += loss.item() * config.accumulation_steps
    
    return total_loss / len(loader)

def evaluate(model, loader, device, scaler=None):
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(loader):
            inputs = batch["input_ids"].to(device, non_blocking=True)
            mask = batch["attention_mask"].to(device, non_blocking=True)
            
            targets = inputs[:, 1:].contiguous()
            inputs = inputs[:, :-1]
            mask = mask[:, :-1]
            
            autocast_context = torch.amp.autocast(
                device_type='cuda', 
                dtype=torch.float16, 
                enabled=scaler is not None and config.mixed_precision
            )
            
            with autocast_context:
                outputs = model(inputs, src_key_padding_mask=~mask)
                outputs = outputs.view(-1, outputs.size(-1))
                targets = targets.view(-1)
                
                loss = F.cross_entropy(
                    outputs,
                    targets,
                    ignore_index=0
                )
            
            total_loss += loss.item()
    
    return total_loss / len(loader)

def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'vocab_size': len(vocab),
        'd_model': config.d_model,
        'nhead': config.nhead,
        'num_layers': config.num_layers,
        'dim_feedforward': config.dim_feedforward,
        'dropout': config.dropout
    }, path)

def run_training(resume_checkpoint=None):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(os.path.dirname(config.model_path), exist_ok=True)
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_texts, val_texts = load_data()
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_texts)} –æ–±—É—á–∞—é—â–∏—Ö —Ñ—Ä–∞–∑, {len(val_texts)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        vocab = build_vocab(train_texts + val_texts)
        print(f"–°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å –∏–∑ {len(vocab)} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        with open(config.vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab, f, ensure_ascii=False)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–æ–≤
        train_dataset = TextDataset(train_texts, vocab, config.max_seq_len)
        val_dataset = TextDataset(val_texts, vocab, config.max_seq_len)
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=min(4, os.cpu_count()),
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=min(2, os.cpu_count()),
            pin_memory=True
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model = TextTransformer(
            vocab_size=len(vocab),
            d_model=config.d_model,
            nhead=config.nhead,
            num_layers=config.num_layers,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout
        ).to(device)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode='min', 
            factor=0.5, 
            patience=2, 
            verbose=True
        )
        
        scaler = None
        if config.mixed_precision and device.type == 'cuda':
            scaler = torch.amp.GradScaler()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        start_epoch = 0
        best_val_loss = float('inf')
        patience_counter = 0
        
        if resume_checkpoint:
            checkpoint = torch.load(resume_checkpoint, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint['val_loss']
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç —ç–ø–æ—Ö–∏ {checkpoint['epoch']}, "
                  f"Train loss: {checkpoint['train_loss']:.4f}, "
                  f"Val loss: {checkpoint['val_loss']:.4f}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        for epoch in range(start_epoch, config.epochs):
            start_time = time.time()
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if device.type == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()
            
            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —ç–ø–æ—Ö–∞
            train_loss = train_epoch(model, train_loader, optimizer, device, scheduler, scaler)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss = evaluate(model, val_loader, device, scaler)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ LR
            scheduler.step(val_loss)
            
            epoch_time = time.time() - start_time
            
            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{config.epochs} | "
                  f"Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | "
                  f"–í—Ä–µ–º—è: {epoch_time:.2f}—Å | LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_loss < best_val_loss - config.min_loss_delta:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'vocab_size': len(vocab),
                    'd_model': config.d_model,
                    'nhead': config.nhead,
                    'num_layers': config.num_layers,
                    'dim_feedforward': config.dim_feedforward,
                    'dropout': config.dropout
                }, config.model_path)
                print(f"üî• –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.model_path}")
                      
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            if val_loss >= best_val_loss - config.min_loss_delta:
                patience_counter += 1
                if patience_counter >= config.early_stop_patience:
                    print(f"üèÅ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                    break
            else:
                save_checkpoint(
                    model, optimizer, epoch+1, 
                    train_loss, val_loss, 
                    config.model_path
                )
                print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {config.model_path}")
                patience_counter = 0
        
        print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è Val loss: {best_val_loss:.4f}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}")
        import traceback
        traceback.print_exc()

# ====================== –ì–ï–ù–ï–†–ê–¶–ò–Ø ======================
def generate_text(model, vocab, prompt, device, max_length=50, temperature=0.7, top_k=50, stop_tokens=None):
    model.eval()
    rev_vocab = {idx: word for word, idx in vocab.items()}
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
    tokens = prompt.split()
    input_ids = [vocab.get(token, vocab["<unk>"]) for token in tokens]
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–æ–≤
    if stop_tokens is None:
        stop_tokens = {"<eos>"}
    stop_ids = {vocab[token] for token in stop_tokens if token in vocab}
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad(), torch.amp.autocast(device_type=device.type, enabled=True):
        for _ in range(max_length):
            inputs = torch.tensor([input_ids], dtype=torch.long).to(device)
            mask = torch.ones_like(inputs, dtype=torch.bool)
            
            outputs = model(inputs, src_key_padding_mask=~mask)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            logits = outputs[0, -1, :] / temperature
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è top-k
            if top_k > 0:
                top_values = torch.topk(logits, top_k)
                indices_to_remove = logits < top_values.values[..., -1, None]
                logits[indices_to_remove] = -float('Inf')
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ softmax
            probs = F.softmax(logits, dim=-1)
            
            # –í—ã–±–æ—Ä–∫–∞
            next_token = torch.multinomial(probs, 1).item()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ã
            if next_token in stop_ids:
                break
                
            input_ids.append(next_token)
            tokens.append(rev_vocab.get(next_token, "<unk>"))
    
    return " ".join(tokens)

# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def interactive_mode():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è
        if not os.path.exists(config.vocab_path):
            print("‚ùå –°–ª–æ–≤–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return
        
        with open(config.vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        if not os.path.exists(config.model_path):
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return
        
        checkpoint = torch.load(config.model_path, map_location=device)
        model = TextTransformer(
            vocab_size=checkpoint['vocab_size'],
            d_model=checkpoint['d_model'],
            nhead=checkpoint['nhead'],
            num_layers=checkpoint['num_layers'],
            dim_feedforward=checkpoint['dim_feedforward'],
            dropout=checkpoint['dropout']
        ).to(device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
        print("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: temperature=0.7, top_k=50")
        
        while True:
            try:
                prompt = input("\n–í–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: ").strip()
            except KeyboardInterrupt:
                print("\n–í—ã—Ö–æ–¥...")
                break
                
            if prompt.lower() == 'exit':
                break
                
            if not prompt:
                print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç!")
                continue
                
            start_time = time.time()
            generated = generate_text(model, vocab, prompt, device)
            gen_time = time.time() - start_time
            
            print(f"\n–°–µ—Ç—å: {generated}")
            print(f"–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {gen_time:.2f}—Å")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ: {str(e)}")

def continue_training():
    run_training(resume_checkpoint=config.model_path)

def main_menu():
    while True:
        print("\n–ú–µ–Ω—é —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏:")
        print("1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è")
        print("2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
        print("3. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
        print("4. –í—ã—Ö–æ–¥")
        choice = input("–í—ã–±–æ—Ä: ").strip()

        if choice == '1':
            run_training()
        elif choice == '2':
            continue_training()
        elif choice == '3':
            interactive_mode()
        elif choice == '4':
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")

if __name__ == "__main__":
    main_menu()