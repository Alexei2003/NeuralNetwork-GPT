import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import os
import math
import time
import numpy as np
from tqdm import tqdm
import torch.nn.functional as F
from collections import Counter
import json
import gc

# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    system = "colab"
    
    if system == "my":
        dir = "/media/alex/Programs/NEURAL_NETWORKS/"
    else:
        dir = "/content/"

    # –ü—É—Ç–∏
    data_path = dir + "NeuralNetwork-GPT/DataSet.txt"
    model_path = dir + "NeuralNetwork-GPT/Model/text_model.pth"
    vocab_path = dir + "NeuralNetwork-GPT/Model/vocab.json"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    d_model = 512
    nhead = 8
    num_layers = 6
    dim_feedforward = 2048
    max_seq_len = 256
    dropout = 0.5

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    batch_size = 32
    lr = 0.0001
    epochs = 100
    accumulation_steps = 4
    early_stop_patience = 3
    min_loss_delta = 0.001
    mixed_precision = True
    factor_lr = 0.5                 # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è learning rate –ø—Ä–∏ plateau
    patience_lr = 0                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è learning rate

config = Config()

# ====================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ======================
class TextDataset(Dataset):
    def __init__(self, texts, vocab, seq_len):
        self.texts = texts
        self.vocab = vocab
        self.seq_len = seq_len
        self.unk_token = vocab["<unk>"]
        self.pad_token = vocab["<pad>"]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        tokens = text.split()[:self.seq_len]
        n_tokens = len(tokens)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∏–Ω–¥–µ–∫—Å—ã
        input_ids = [self.vocab.get(token, self.unk_token) for token in tokens]

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞
        if n_tokens < self.seq_len:
            pad_len = self.seq_len - n_tokens
            input_ids += [self.pad_token] * pad_len
            attention_mask = [1] * n_tokens + [0] * pad_len
        else:
            attention_mask = [1] * self.seq_len

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.bool)
        }

def build_vocab(texts, min_freq=3):
    counter = Counter()
    for text in texts:
        counter.update(text.split())

    vocab = {
        "<pad>": 0,
        "<unk>": 1,
        "<sos>": 2,
        "<eos>": 3
    }

    idx = 4
    for word, count in counter.items():
        if count >= min_freq:
            vocab[word] = idx
            idx += 1

    return vocab

def load_data(val_split=0.1):
    if not os.path.exists(config.data_path):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config.data_path}")

    with open(config.data_path, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]

    if len(texts) == 0:
        raise ValueError(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—É—Å—Ç: {config.data_path}")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    val_size = int(len(texts) * val_split)
    train_size = len(texts) - val_size
    train_texts, val_texts = random_split(texts, [train_size, val_size])

    return list(train_texts), list(val_texts)

# ====================== –ú–û–î–ï–õ–¨ ======================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(1), :].squeeze(1)
        return self.dropout(x)

class TextTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout, config.max_seq_len)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model, nhead, dim_feedforward, dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, src, src_key_padding_mask=None):
        src = self.embedding(src)
        src = self.pos_encoder(src)
        output = self.transformer(src, src_key_padding_mask=src_key_padding_mask)
        return self.fc_out(output)

# ====================== –û–ë–£–ß–ï–ù–ò–ï ======================
def train_epoch(model, loader, optimizer, device, scheduler=None, scaler=None):
    model.train()
    total_loss = 0.0
    optimizer.zero_grad()

    for i, batch in enumerate(tqdm(loader)):
        inputs = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)

        # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã (—Å–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω)
        targets = inputs[:, 1:].contiguous()
        inputs = inputs[:, :-1]
        mask = mask[:, :-1]

        autocast_context = torch.amp.autocast(
            device_type='cuda' if device.type == 'cuda' else 'cpu',
            dtype=torch.float16,
            enabled=scaler is not None and config.mixed_precision
        )

        with autocast_context:
            outputs = model(inputs, src_key_padding_mask=~mask)
            outputs = outputs.view(-1, outputs.size(-1))
            targets = targets.view(-1)

            loss = F.cross_entropy(
                outputs,
                targets,
                ignore_index=0
            ) / config.accumulation_steps

        if scaler:
            scaler.scale(loss).backward()
        else:
            loss.backward()

        if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(loader):
            if scaler:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

            optimizer.zero_grad(set_to_none=True)

        total_loss += loss.item() * config.accumulation_steps

    return total_loss / len(loader)

@torch.inference_mode()
def evaluate(model, loader, device, scaler=None):
    model.eval()
    total_loss = 0.0

    for batch in tqdm(loader):
        inputs = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)

        targets = inputs[:, 1:].contiguous()
        inputs = inputs[:, :-1]
        mask = mask[:, :-1]

        autocast_context = torch.amp.autocast(
            device_type='cuda' if device.type == 'cuda' else 'cpu',
            dtype=torch.float16,
            enabled=scaler is not None and config.mixed_precision
        )

        with autocast_context:
            outputs = model(inputs, src_key_padding_mask=~mask)
            outputs = outputs.view(-1, outputs.size(-1))
            targets = targets.view(-1)

            loss = F.cross_entropy(
                outputs,
                targets,
                ignore_index=0
            )

        total_loss += loss.item()

    return total_loss / len(loader)

def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, vocab, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'vocab_size': len(vocab),
        'd_model': config.d_model,
        'nhead': config.nhead,
        'num_layers': config.num_layers,
        'dim_feedforward': config.dim_feedforward,
        'dropout': config.dropout
    }, path)

def run_training(resume_checkpoint=None):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(os.path.dirname(config.model_path), exist_ok=True)

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_texts, val_texts = load_data()
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_texts)} –æ–±—É—á–∞—é—â–∏—Ö —Ñ—Ä–∞–∑, {len(val_texts)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        all_texts = train_texts + val_texts  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
        vocab = build_vocab(all_texts)
        print(f"–°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å –∏–∑ {len(vocab)} —Ç–æ–∫–µ–Ω–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        with open(config.vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab, f, ensure_ascii=False)

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–æ–≤
        train_dataset = TextDataset(train_texts, vocab, config.max_seq_len)
        val_dataset = TextDataset(val_texts, vocab, config.max_seq_len)

        train_loader = DataLoader(
            train_dataset,
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=min(4, os.cpu_count()),
            pin_memory=True,
            persistent_workers=True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=min(2, os.cpu_count()),
            pin_memory=True,
            persistent_workers=True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model = TextTransformer(
            vocab_size=len(vocab),
            d_model=config.d_model,
            nhead=config.nhead,
            num_layers=config.num_layers,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout
        ).to(device)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=config.factor_lr,
            patience=config.patience_lr,
            verbose=True
        )

        scaler = None
        if config.mixed_precision and device.type == 'cuda':
            scaler = torch.amp.GradScaler('cuda', enabled=config.mixed_precision and torch.cuda.is_available())

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è
        start_epoch = 0
        best_val_loss = float('inf')
        patience_counter = 0

        # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        if resume_checkpoint and os.path.exists(resume_checkpoint):
            checkpoint = torch.load(resume_checkpoint, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint['val_loss']
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç —ç–ø–æ—Ö–∏ {checkpoint['epoch']}, "
                  f"Train loss: {checkpoint['train_loss']:.4f}, "
                  f"Val loss: {checkpoint['val_loss']:.4f}")

        # –û–±—É—á–µ–Ω–∏–µ
        for epoch in range(start_epoch, config.epochs):
            epoch_start = time.time()

            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if device.type == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()

            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —ç–ø–æ—Ö–∞
            train_loss = train_epoch(model, train_loader, optimizer, device, scheduler, scaler)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss = evaluate(model, val_loader, device, scaler)
            scheduler.step(val_loss)

            epoch_time = time.time() - epoch_start
            lr = optimizer.param_groups[0]['lr']

            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{config.epochs} | "
                  f"Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | "
                  f"–í—Ä–µ–º—è: {epoch_time:.2f}—Å | LR: {lr:.6f}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_loss < best_val_loss - config.min_loss_delta:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(model.state_dict(), config.model_path)
                print(f"üî• –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.model_path}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –∫–∞–∂–¥—ã–π —ç–ø–æ—Ö
            save_checkpoint(model, optimizer, epoch, train_loss, val_loss, vocab, config.model_path)
            print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {config.model_path}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if val_loss >= best_val_loss - config.min_loss_delta:
                patience_counter += 1
                if patience_counter >= config.early_stop_patience:
                    print(f"üèÅ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                    break

        print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è Val loss: {best_val_loss:.4f}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}")
        import traceback
        traceback.print_exc()

# ====================== –ì–ï–ù–ï–†–ê–¶–ò–Ø ======================
@torch.inference_mode()
def generate_text(model, vocab, prompt, device, max_length=50, temperature=0.7, top_k=50, stop_tokens=None):
    model.eval()
    rev_vocab = {idx: word for word, idx in vocab.items()}

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
    tokens = prompt.split()
    input_ids = [vocab.get(token, vocab["<unk>"]) for token in tokens]

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–æ–≤ (–≤–∫–ª—é—á–∞—è <unk>)
    stop_tokens = stop_tokens or {"<eos>"}
    stop_tokens = stop_tokens | {"<unk>"}  # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤
    stop_ids = {vocab[token] for token in stop_tokens if token in vocab}

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    for _ in range(max_length):
        inputs = torch.tensor([input_ids], dtype=torch.long).to(device)
        mask = torch.ones_like(inputs, dtype=torch.bool)

        with torch.amp.autocast(device_type=device.type, enabled=True):
            outputs = model(inputs, src_key_padding_mask=~mask)
            logits = outputs[0, -1, :] / temperature

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è top-k
            if top_k > 0:
                top_vals, top_idxs = torch.topk(logits, top_k)
                logits[logits < top_vals[-1]] = -float('Inf')

            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, 1).item()

            # –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–∞ (–≤–∫–ª—é—á–∞—è <unk>)
            if next_token in stop_ids:
                break

            input_ids.append(next_token)
            tokens.append(rev_vocab.get(next_token, "<unk>"))

    return " ".join(tokens)

# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def interactive_mode():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è
        if not os.path.exists(config.vocab_path):
            print("‚ùå –°–ª–æ–≤–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return

        with open(config.vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        if not os.path.exists(config.model_path):
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        model_path = config.model_path if os.path.exists(config.model_path) else config.model_path
        checkpoint = torch.load(model_path, map_location=device)

        model = TextTransformer(
            vocab_size=checkpoint.get('vocab_size', len(vocab)),
            d_model=checkpoint.get('d_model', config.d_model),
            nhead=checkpoint.get('nhead', config.nhead),
            num_layers=checkpoint.get('num_layers', config.num_layers),
            dim_feedforward=checkpoint.get('dim_feedforward', config.dim_feedforward),
            dropout=checkpoint.get('dropout', config.dropout)
        ).to(device)

        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
        print("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: temperature=0.7, top_k=50")

        while True:
            try:
                prompt = input("\n–í–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: ").strip()
                if not prompt:
                    print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç!")
                    continue
                if prompt.lower() == 'exit':
                    break

                start_time = time.time()
                generated = generate_text(model, vocab, prompt, device)
                gen_time = time.time() - start_time

                print(f"\n–°–µ—Ç—å: {generated}")
                print(f"–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {gen_time:.2f}—Å")

            except KeyboardInterrupt:
                print("\n–í—ã—Ö–æ–¥...")
                break

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ: {str(e)}")

def continue_training():
    run_training(resume_checkpoint=config.model_path)

def main_menu():
    while True:
        print("\n–ú–µ–Ω—é —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏:")
        print("1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è")
        print("2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
        print("3. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
        print("4. –í—ã—Ö–æ–¥")
        choice = input("–í—ã–±–æ—Ä: ").strip()

        if choice == '1':
            run_training()
        elif choice == '2':
            continue_training()
        elif choice == '3':
            interactive_mode()
        elif choice == '4':
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")

if __name__ == "__main__":
    main_menu()"

    if system == "my":
        dir = "/media/alex/Programs/NEURAL_NETWORKS/"
    else:
        dir = "/content/"

    # –ü—É—Ç–∏
    data_path = dir + "NeuralNetwork-GPT/DataSet.txt"
    model_path = dir + "NeuralNetwork-GPT/Model/text_model.pth"
    vocab_path = dir + "NeuralNetwork-GPT/Model/vocab.json"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    d_model = 512
    nhead = 8
    num_layers = 6
    dim_feedforward = 2048
    max_seq_len = 256
    dropout = 0.5

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    batch_size = 32
    lr = 0.0001
    epochs = 100
    accumulation_steps = 4
    early_stop_patience = 3
    min_loss_delta = 0.001
    mixed_precision = True
    factor_lr = 0.5                 # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è learning rate –ø—Ä–∏ plateau
    patience_lr = 0                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è learning rate

config = Config()

# ====================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ======================
class TextDataset(Dataset):
    def __init__(self, texts, vocab, seq_len):
        self.texts = texts
        self.vocab = vocab
        self.seq_len = seq_len
        self.unk_token = vocab["<unk>"]
        self.pad_token = vocab["<pad>"]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        tokens = text.split()[:self.seq_len]
        n_tokens = len(tokens)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∏–Ω–¥–µ–∫—Å—ã
        input_ids = [self.vocab.get(token, self.unk_token) for token in tokens]

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞
        if n_tokens < self.seq_len:
            pad_len = self.seq_len - n_tokens
            input_ids += [self.pad_token] * pad_len
            attention_mask = [1] * n_tokens + [0] * pad_len
        else:
            attention_mask = [1] * self.seq_len

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.bool)
        }

def build_vocab(texts, min_freq=3):
    counter = Counter()
    for text in texts:
        counter.update(text.split())

    vocab = {
        "<pad>": 0,
        "<unk>": 1,
        "<sos>": 2,
        "<eos>": 3
    }

    idx = 4
    for word, count in counter.items():
        if count >= min_freq:
            vocab[word] = idx
            idx += 1

    return vocab

def load_data(val_split=0.1):
    if not os.path.exists(config.data_path):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config.data_path}")

    with open(config.data_path, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]

    if len(texts) == 0:
        raise ValueError(f"‚ùå –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—É—Å—Ç: {config.data_path}")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    val_size = int(len(texts) * val_split)
    train_size = len(texts) - val_size
    train_texts, val_texts = random_split(texts, [train_size, val_size])

    return list(train_texts), list(val_texts)

# ====================== –ú–û–î–ï–õ–¨ ======================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(1), :].squeeze(1)
        return self.dropout(x)

class TextTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout, config.max_seq_len)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model, nhead, dim_feedforward, dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, src, src_key_padding_mask=None):
        src = self.embedding(src)
        src = self.pos_encoder(src)
        output = self.transformer(src, src_key_padding_mask=src_key_padding_mask)
        return self.fc_out(output)

# ====================== –û–ë–£–ß–ï–ù–ò–ï ======================
def train_epoch(model, loader, optimizer, device, scheduler=None, scaler=None):
    model.train()
    total_loss = 0.0
    optimizer.zero_grad()

    for i, batch in enumerate(tqdm(loader)):
        inputs = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)

        # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã (—Å–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω)
        targets = inputs[:, 1:].contiguous()
        inputs = inputs[:, :-1]
        mask = mask[:, :-1]

        autocast_context = torch.amp.autocast(
            device_type='cuda' if device.type == 'cuda' else 'cpu',
            dtype=torch.float16,
            enabled=scaler is not None and config.mixed_precision
        )

        with autocast_context:
            outputs = model(inputs, src_key_padding_mask=~mask)
            outputs = outputs.view(-1, outputs.size(-1))
            targets = targets.view(-1)

            loss = F.cross_entropy(
                outputs,
                targets,
                ignore_index=0
            ) / config.accumulation_steps

        if scaler:
            scaler.scale(loss).backward()
        else:
            loss.backward()

        if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(loader):
            if scaler:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

            optimizer.zero_grad(set_to_none=True)

        total_loss += loss.item() * config.accumulation_steps

    return total_loss / len(loader)

@torch.inference_mode()
def evaluate(model, loader, device, scaler=None):
    model.eval()
    total_loss = 0.0

    for batch in tqdm(loader):
        inputs = batch["input_ids"].to(device, non_blocking=True)
        mask = batch["attention_mask"].to(device, non_blocking=True)

        targets = inputs[:, 1:].contiguous()
        inputs = inputs[:, :-1]
        mask = mask[:, :-1]

        autocast_context = torch.amp.autocast(
            device_type='cuda' if device.type == 'cuda' else 'cpu',
            dtype=torch.float16,
            enabled=scaler is not None and config.mixed_precision
        )

        with autocast_context:
            outputs = model(inputs, src_key_padding_mask=~mask)
            outputs = outputs.view(-1, outputs.size(-1))
            targets = targets.view(-1)

            loss = F.cross_entropy(
                outputs,
                targets,
                ignore_index=0
            )

        total_loss += loss.item()

    return total_loss / len(loader)

def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, vocab, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'vocab_size': len(vocab),
        'd_model': config.d_model,
        'nhead': config.nhead,
        'num_layers': config.num_layers,
        'dim_feedforward': config.dim_feedforward,
        'dropout': config.dropout
    }, path)

def run_training(resume_checkpoint=None):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(os.path.dirname(config.model_path), exist_ok=True)

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_texts, val_texts = load_data()
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_texts)} –æ–±—É—á–∞—é—â–∏—Ö —Ñ—Ä–∞–∑, {len(val_texts)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö")

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        all_texts = train_texts + val_texts  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
        vocab = build_vocab(all_texts)
        print(f"–°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å –∏–∑ {len(vocab)} —Ç–æ–∫–µ–Ω–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        with open(config.vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab, f, ensure_ascii=False)

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–æ–≤
        train_dataset = TextDataset(train_texts, vocab, config.max_seq_len)
        val_dataset = TextDataset(val_texts, vocab, config.max_seq_len)

        train_loader = DataLoader(
            train_dataset,
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=min(4, os.cpu_count()),
            pin_memory=True,
            persistent_workers=True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=min(2, os.cpu_count()),
            pin_memory=True,
            persistent_workers=True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model = TextTransformer(
            vocab_size=len(vocab),
            d_model=config.d_model,
            nhead=config.nhead,
            num_layers=config.num_layers,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout
        ).to(device)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=config.factor_lr,
            patience=config.patience_lr,
            verbose=True
        )

        scaler = None
        if config.mixed_precision and device.type == 'cuda':
            scaler = torch.amp.GradScaler('cuda', enabled=config.mixed_precision and torch.cuda.is_available())

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è
        start_epoch = 0
        best_val_loss = float('inf')
        patience_counter = 0

        # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        if resume_checkpoint and os.path.exists(resume_checkpoint):
            checkpoint = torch.load(resume_checkpoint, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint['val_loss']
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç —ç–ø–æ—Ö–∏ {checkpoint['epoch']}, "
                  f"Train loss: {checkpoint['train_loss']:.4f}, "
                  f"Val loss: {checkpoint['val_loss']:.4f}")

        # –û–±—É—á–µ–Ω–∏–µ
        for epoch in range(start_epoch, config.epochs):
            epoch_start = time.time()

            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if device.type == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()

            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —ç–ø–æ—Ö–∞
            train_loss = train_epoch(model, train_loader, optimizer, device, scheduler, scaler)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss = evaluate(model, val_loader, device, scaler)
            scheduler.step(val_loss)

            epoch_time = time.time() - epoch_start
            lr = optimizer.param_groups[0]['lr']

            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{config.epochs} | "
                  f"Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | "
                  f"–í—Ä–µ–º—è: {epoch_time:.2f}—Å | LR: {lr:.6f}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_loss < best_val_loss - config.min_loss_delta:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(model.state_dict(), config.model_path)
                print(f"üî• –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.model_path}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –∫–∞–∂–¥—ã–π —ç–ø–æ—Ö
            save_checkpoint(model, optimizer, epoch, train_loss, val_loss, vocab, config.model_path)
            print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {config.model_path}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if val_loss >= best_val_loss - config.min_loss_delta:
                patience_counter += 1
                if patience_counter >= config.early_stop_patience:
                    print(f"üèÅ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                    break

        print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è Val loss: {best_val_loss:.4f}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}")
        import traceback
        traceback.print_exc()

# ====================== –ì–ï–ù–ï–†–ê–¶–ò–Ø ======================
@torch.inference_mode()
def generate_text(model, vocab, prompt, device, max_length=50, temperature=0.7, top_k=50, stop_tokens=None):
    model.eval()
    rev_vocab = {idx: word for word, idx in vocab.items()}

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
    tokens = prompt.split()
    input_ids = [vocab.get(token, vocab["<unk>"]) for token in tokens]

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–æ–≤ (–≤–∫–ª—é—á–∞—è <unk>)
    stop_tokens = stop_tokens or {"<eos>"}
    stop_tokens = stop_tokens | {"<unk>"}  # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤
    stop_ids = {vocab[token] for token in stop_tokens if token in vocab}

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    for _ in range(max_length):
        inputs = torch.tensor([input_ids], dtype=torch.long).to(device)
        mask = torch.ones_like(inputs, dtype=torch.bool)

        with torch.amp.autocast(device_type=device.type, enabled=True):
            outputs = model(inputs, src_key_padding_mask=~mask)
            logits = outputs[0, -1, :] / temperature

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è top-k
            if top_k > 0:
                top_vals, top_idxs = torch.topk(logits, top_k)
                logits[logits < top_vals[-1]] = -float('Inf')

            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, 1).item()

            # –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–∞ (–≤–∫–ª—é—á–∞—è <unk>)
            if next_token in stop_ids:
                break

            input_ids.append(next_token)
            tokens.append(rev_vocab.get(next_token, "<unk>"))

    return " ".join(tokens)

# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def interactive_mode():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è
        if not os.path.exists(config.vocab_path):
            print("‚ùå –°–ª–æ–≤–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return

        with open(config.vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        if not os.path.exists(config.model_path):
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        model_path = config.model_path if os.path.exists(config.model_path) else config.model_path
        checkpoint = torch.load(model_path, map_location=device)

        model = TextTransformer(
            vocab_size=checkpoint.get('vocab_size', len(vocab)),
            d_model=checkpoint.get('d_model', config.d_model),
            nhead=checkpoint.get('nhead', config.nhead),
            num_layers=checkpoint.get('num_layers', config.num_layers),
            dim_feedforward=checkpoint.get('dim_feedforward', config.dim_feedforward),
            dropout=checkpoint.get('dropout', config.dropout)
        ).to(device)

        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
        print("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: temperature=0.7, top_k=50")

        while True:
            try:
                prompt = input("\n–í–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: ").strip()
                if not prompt:
                    print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç!")
                    continue
                if prompt.lower() == 'exit':
                    break

                start_time = time.time()
                generated = generate_text(model, vocab, prompt, device)
                gen_time = time.time() - start_time

                print(f"\n–°–µ—Ç—å: {generated}")
                print(f"–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {gen_time:.2f}—Å")

            except KeyboardInterrupt:
                print("\n–í—ã—Ö–æ–¥...")
                break

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ: {str(e)}")

def continue_training():
    run_training(resume_checkpoint=config.model_path)

def main_menu():
    while True:
        print("\n–ú–µ–Ω—é —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏:")
        print("1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è")
        print("2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
        print("3. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
        print("4. –í—ã—Ö–æ–¥")
        choice = input("–í—ã–±–æ—Ä: ").strip()

        if choice == '1':
            run_training()
        elif choice == '2':
            continue_training()
        elif choice == '3':
            interactive_mode()
        elif choice == '4':
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")

if __name__ == "__main__":
    main_menu()